---
title: "Predictive Modeling Process"
author: "Abbey Chaver and Tina Huang"
output: ioslides_presentation
---
```{r setup, include=FALSE, echo=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(pander)
data <- read.csv("../data/datasets/Credit.csv", header=TRUE, row.names = 1)
```

## Introduction
- Credit.csv dataset
```{r, echo=FALSE}
panderOptions('table.split.table',60)
pander(head(data, 2))
```

- Want to predict `Balance` of an account, given ten predictors

# Methods

## Exploration of Variables

- Quantitative  
    - Summary table of max, min, range, median, 1st and 3rd quartiles, IQR, mean, and sd  
    - Histograms  
    - Boxplots
  
- Qualitative
    - Frequency and Relative Frequency Tables
    - Barplots of Frequencies
    - Conditional Boxplots between Balance and the qualitative variable
  
## Example: Income
<div class="columns-2">
  <img src="../images/Boxplot_Income.png" alt="Income Boxplot" height="400" width="400">
  
  <img src="../images/Histogram_Income.png" alt="Income Histogram" height="400" width="400">
</div>

## Example: Student (yes or no)

<div class="columns-2">
  <img src="../images/Barplot_Student.png" alt="Student Barplot" height="400" width="400">
  
  <img src="../images/Conditional_Boxplot_Balance_Student.png" alt="Balance Student Boxplot" height="400" width="400">
</div>

## Identifying Correlations
<div class="columns-2", style="margins:auto">
  <img src="../images/correlation_scatterplot.png" alt="Correlation Matrix plot" height="400" width="400">
</div>

- `Income`, `Limit`, and `Rating` are correlated with `Balance`
- Highly correlated with each other

## Preparing the Data

- Dummy out the categorical variables

```
new_data <- model.matrix(Balance ~ ., data=data)
new_data <- cbind(new_data[ ,-1], Balance = data$Balance)
```

- Mean-center and standardize all variables

```
scaled_data <- scale(new_data, center = TRUE, scale = TRUE)
```

- Divide scaled data into training and test sets

# Regression Models

## Ordinary Least Squares
- Use `lm()` function
- Then find the coefficients and mse of the model

```
ols = lm(Balance~Income+Limit+Rating+Cards+Age+Education+GenderFemale
+StudentYes+MarriedYes+EthnicityAsian+EthnicityCaucasian, data=scaled_data)

ols_coeff = ols$coefficients[-1]

ols_mse = mean(ols$residuals^2)
```

## Ridge
<div class="columns-2">
  <img src="../images/Ridge_MSE.png" alt="Ridge MSE" height="400" width="400">
  
  - Use `glmnet` R package
  
  - First perform cross validation on the training set to find the value of lambda that results in the lowest cross validation error

  - Smallest value of lambda is optimal
  
</div>



## Ridge: Continued

- Find mean squared error of the best model with this value of lambda on the test set

```
grid <- grid <- 10^(seq(10, -2, length = 100))
cv <- cv.glmnet(trainX, trainY, intercept = FALSE, standardize = FALSE, 
lambda = grid, alpha=0)
lambda = cv$lambda.min

predictedY <- predict(cv, newx = testX, s = "lambda.min")
ssq <- sum((predictedY - testY)^2)
mse = ssq/nrow(testX)
```
- Using that value of lambda, refit the model on the whole dataset to find the official coefficients

```
fcv <- glmnet(dataX, dataY, intercept = FALSE, standardize = FALSE, 
lambda = lambda,alpha=0)
```

## Lasso

<div class="columns-2">
  <img src="../images/Lasso_MSE.png" alt="Lasso MSE" height="400" width="400">

  - Similar to Ridge, first perform cross validation to find best value of lambda
  
  - Smallest value selected again
  
</div>

## Lasso Continued
```
cv_out = cv.glmnet(train_x, train_y, alpha=1, intercept=FALSE, standardize=FALSE,
lambda=grid)
lasso_lambda = cv_out$lambda.min
```
- Find the mse, then refit the model to find the official coefficients

```
lasso_pred = predict(lasso_mod, s = lasso_lambda, newx=test_x)
lasso_mse = mean((lasso_pred-test_y)^2)
lasso_out = glmnet(scaled_x, scaled_y, alpha=1, lambda=grid)
lasso_coeff = predict(lasso_out,type="coefficients",s=lasso_lambda)[2:12,]
```

## Principal Components

<div class="columns-2">
  <img src="../images/pcr_validationplot.png" alt="pcr components plot" height="400" width="400">
  
  
  - Use `pls` R package
  
  - Peform 10-fold cross-validation on training set to find optimal number of components (12)
  
  - Fit model on full dataset to get final coefficients

</div>

```
pcr_model <- pcr(Balance ~ ., data=training, validation = "CV")
pcr_comp = which.min(pcr_model$validation$PRESS)
full_pcr_model <- pcr(Balance ~ ., data=data)
best_coefs <- full_pcr_model$coefficients[, , pcr_comp][-1]
```


## Partial Least Squares
<div class="columns-2">
  <img src="../images/plsr_validationplot.png" alt="pcr components plot" height="400" width="400">

  - Use `pls` package again

  - Perform 10-fold cross validation to find the best number of components (6)
  
  - Fit the model with this number of components to the whole dataset and find the coefficients
</div>
```
pls_fit = plsr(Balance ~ ., data=training_data,scale=TRUE, validation="CV")
pls_comp = which.min(pls_fit$validation$PRESS)
pls_pred = predict(pls_fit, test_x, ncomp=pls_comp)
pls_mse = mean((pls_pred-test_y)^2)
```

# Comparison of All Models

## Plot of Coefficients

<img src="../images/Coefficients_Plot.png" alt="Coefficients Plot" height="500" width="800">

## Table of MSEs

```{r echo=FALSE}
load("../data/results.Rdata")
panderOptions('round', 5)
pander(mse_table, caption="MSE for Different Models")

```

- We can see that all of the mses are fairly close together, with ridge producing the lowest mse by a small amount.

# Thanks!

